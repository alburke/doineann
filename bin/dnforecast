#!/usr/bin/env python
from sklearn.preprocessing import OneHotEncoder
from processing.DLModeler import DLModeler
from util.Config import Config
from tensorflow.keras import models
from multiprocessing import Pool
import multiprocessing as mp
import argparse, pdb
import pandas as pd
import numpy as np
import h5py
import os
np.random.seed(123)  # for reproducibility

def main():
    """
    Main function to parse out configuration file (Config), a dictionary 
    of different model tunnings, for slicing model and observational data. 

    For a given number of parallel processesors, the model and observational 
    data are sliced each day with the model data separated by ensemble member. 
    """
    parser = argparse.ArgumentParser("hsdata - Hagelslag Data Processor")
    parser.add_argument("config", help="Configuration file")
    parser.add_argument("-t", "--train", action="store_true", help="Train machine learning models.")
    parser.add_argument("-f", "--fore", action="store_true", help="Generate forecasts from machine learning models.")
    args = parser.parse_args()
    required = ['start_dates','end_dates','start_hour','end_hour','ensemble_members',
                'model_path','ensemble_name','storm_variables','potential_variables', 
                'model_map_file','hf_path','forecast_grid_path','patch_radius',
                'num_examples','class_percentages'] 
    
    #Add attributes of dict to config
    config = Config(args.config, required_attributes=required)
    config.valid_hours = np.arange(config.start_hour, config.end_hour)
    if not hasattr(config, "run_date_format"):
        config.run_date_format = "%Y%m%d-%H"
    config.forecast_variables = config.storm_variables + config.potential_variables
    if hasattr(config, "tendency_variables"):
        config.forecast_variables.extend(config.tendency_variables)
    short_variable_names = []
    for variable in config.forecast_variables:
        if " " in variable: 
            variable_name=''.join([v[0].upper() for v in variable.split()]) + variable.split('_')[-1]
        elif "_" in variable: 
            variable_name= ''.join([v[0].upper() for v in variable.split()]) + variable.split('_')[-1]
        else:
            variable_name = variable
        short_variable_names.append(variable_name)
    
    #Process data for different processor arguments
    dlmodel = DLModeler(config.model_path,config.hf_path,
        config.start_dates,config.end_dates,config.num_examples,
        config.class_percentages,config.patch_radius,
        config.run_date_format,np.array(short_variable_names)) 
    
    if args.train:
        for member in config.ensemble_members:
            train_models(config,dlmodel,member)
    elif args.fore:
        forecast_dates = pd.date_range(start=config.start_dates['forecast'],
            end=config.end_dates['forecast'],freq='1D').strftime(config.run_date_format)
        for date in forecast_dates:
            for member in config.ensemble_members:
                patch_data = predict_models(config,dlmodel,member,date)
                gridded_output(config,dlmodel,member,date,patch_data)
    return

def train_models(config,dlmodel,member):
    """
    Function that reads and extracts pre-processed 2d member data 
    from an ensemble to train a convolutional neural net (cnn). 
    The model data is standardized before being input to the cnn, 
    with the observation data in the shape (# examples, # classes). 

    Args:
        config (object): configuration object 
        dlmodel (object): class object for creating/processing data for a cnn
        member (str): ensemble member data that trains a cnn
    """
    train_data, train_label = dlmodel.preprocess_training_data(member)
    valid_data, valid_label = dlmodel.preprocess_validation_data(member)
    
    onehot_encoder = OneHotEncoder(sparse=False,categories='auto')
    if train_data is not None and valid_data is not None:
        #Train and save models predicting severe hail or no hail
        train_obs_label = onehot_encoder.fit_transform(train_label.reshape(-1, 1))
        dlmodel.train_CNN(member,train_data,train_obs_label,valid_data,valid_label)
    return 


def predict_models(config,dlmodel,member,date):
    """
    Function that opens a pre-trained convolutional neural net (cnn). 
    and predicts hail probability forecasts for a single ensemble member.
    
    Args:
        config (object): configuration object 
        dlmodel (object): class object for creating/processing data for a cnn
        member (str): ensemble member data that trains a cnn
 
    """
    #Read data on forecast days
    print('\nPredicting {0} {1} data'.format(member,date))
    forecast_data = dlmodel.read_files('forecast',member,date)
    
    #Extract forecast data (#hours, #patches, nx, ny, #variables)
    if forecast_data is None: 
        print('No forecast data found')
        return
    #Open CNN model 
    cnn_model_file = config.model_path+'/{0}_{1}_{2}_{3}_CNN_model.h5'.format(
            member,config.start_dates['train'].strftime('%Y%m%d'),
            config.end_dates['train'].strftime('%Y%m%d'),config.num_examples)
    #Load saved CNN model
    cnn_model = models.load_model(cnn_model_file) 
    
    patch_predictions = np.empty( (2,forecast_data.shape[0],forecast_data.shape[1]) )*np.nan
    #Produce hail forecast using standardized forecast data every hour
    reshaped_forecast_data = forecast_data.reshape(-1,*forecast_data.shape[-3:])
    standard_forecast_data = dlmodel.standardize_data(member,reshaped_forecast_data) 
    print(np.shape(standard_forecast_data))
    #Predict probability of severe hail
    cnn_preds = cnn_model.predict(standard_forecast_data)
    
    #Use minimum prob threshold chosen with validation data
    threshold_model_file = config.model_path+'/{0}_{1}_{2}_{3}_CNN_model_threshold.h5'.format(
            member,config.start_dates['train'].strftime('%Y%m%d'),
            config.end_dates['train'].strftime('%Y%m%d'),config.num_examples)
    
    if not os.path.exists(prob_model_file):
        print('No thresholds found')
        return 
    prob_thresholds = pd.read_csv(threshold_model_file)
    threshold_sev_probs = np.where( (cnn_preds[:,2]+cnn_preds[:,3]) >= prob_thresholds.loc[0,'size_threshold'], 1, 0)
    threshold_sig_probs = np.where(
                ((cnn_preds[:,2]+cnn_preds[:,3]) >= prob_thresholds.loc[0,'size_threshold']) &
                (cnn_preds[:,3] > cnn_preds[:,2]), 1,0)
    
    print('No. severe probs',np.count_nonzero(threshold_sev_probs))
    print('No. sig severe probs',np.count_nonzero(threshold_sig_probs))
    
    patch_predictions = np.array([threshold_sev_probs.reshape(forecast_data.shape[0],forecast_data.shape[1]),
        threshold_sig_probs.reshape(forecast_data.shape[0],forecast_data.shape[1])])
    return patch_predictions

def gridded_output(config,dlmodel,member,date,patch_data):
    #Output gridded forecasts
    date_outpath = config.forecast_grid_path + '{0}/'.format(date[:-5])
    if not os.path.exists(date_outpath): os.makedirs(date_outpath)
    gridded_out_file = date_outpath + '{0}_{1}_forecast_grid.h5'.format(member,date)
    gridded_data = dlmodel.gridded_forecasts(patch_data,config.model_map_file)    
    print('Writing out {0}'.format(gridded_out_file))
    with h5py.File(gridded_out_file, 'w') as hf:
        hf.create_dataset("data",data=gridded_data,
            compression='gzip',compression_opts=6)
    return

if __name__ == "__main__":
    __spec__ = None
    main()

